# Retrieval Optimization: Tokenization to Vector Quantization

[![DeepLearning.AI](https://img.shields.io/badge/Powered%20By-DeepLearning.AI-blueviolet?logo=ai)](https://www.deeplearning.ai/)
[![Qdrant](https://img.shields.io/badge/Vector%20Search-Qdrant-orange?logo=qdrant)](https://qdrant.tech/)

> A hands-on repository for the **Deeplearning.AI x Qdrant course**: *Retrieval Optimization: Tokenization to Vector Quantization*.
> Learn how embedding models, tokenization, search relevance, and vector quantization come together to power **state-of-the-art retrieval systems**.

---

## ðŸ“– Table of Contents

* [About the Course](#about-the-course)
* [Learning Objectives](#learning-objectives)
* [Course Modules](#course-modules)
* [Getting Started](#getting-started)
* [Acknowledgments](#acknowledgments)

---

## ðŸŽ¯ About the Course

This repo complements the **Retrieval Optimization** course by [Deeplearning.AI](https://www.deeplearning.ai/) in collaboration with [Qdrant](https://qdrant.tech/).
It is designed for students and practitioners who want to **optimize retrieval systems** by understanding the role of tokenization, embeddings, vector search algorithms, and quantization techniques.

---

## âœ… Learning Objectives

By the end of this course, you will be able to:

* Understand how **embedding models** represent text for retrieval.
* Explore the **role of tokenizers** and their practical implications.
* Measure **search relevance** and evaluate retrieval performance.
* Optimize **HNSW (Hierarchical Navigable Small World) search** for scalability.
* Apply **vector quantization techniques** (product, scalar, binary) and analyze their trade-offs.

---

## ðŸ“š Course Modules

### 1. Embedding Models

* Introduction to embedding-based retrieval.
* How embeddings capture semantic meaning.

### 2. Role of Tokenizers

* Tokenization basics and challenges.
* Why different tokenizers affect retrieval.

### 3. Practical Implications of Tokenization

* Impact of token length and vocabulary.
* Trade-offs between speed and accuracy.

### 4. Measuring Search Relevance

* Metrics for retrieval quality (e.g., Recall, Precision, nDCG).
* Evaluating embeddings in real-world tasks.

### 5. Optimizing HNSW Search

* Fundamentals of HNSW graphs.
* Tuning parameters for efficiency and accuracy.

### 6. Vector Quantization

* **Product Quantization** â€“ compression with multi-dimensional subspaces.
* **Scalar Quantization** â€“ simple yet effective value-level compression.
* **Binary Quantization** â€“ lightweight, fast, but lossy.
* Comparative analysis of pros & cons.

---

## âš¡ Getting Started

### Prerequisites

* Python 3.8+
* Jupyter Notebook / Google Colab
* Recommended: GPU for faster experiments

### Installation

```bash
# Clone the repository
git clone https://github.com/sdivyanshu90/Retrieval-Optimization-Tokenization-to-Vector-Quantization.git
cd Retrieval-Optimization-Tokenization-to-Vector-Quantization

# (Optional) create a virtual environment
python -m venv venv
source venv/bin/activate   # On Windows use venv\Scripts\activate
```

---

## ðŸ™Œ Acknowledgments

This course and repository are made possible thanks to:

* [**Deeplearning.AI**](https://www.deeplearning.ai/) for designing world-class AI education.
* [**Qdrant**](https://qdrant.tech/) for enabling cutting-edge vector search solutions.

All rights, course content, and licensing belong to **Deeplearning.AI**.

---

âš¡ *Happy Learning, and may your vectors always be relevant!*
